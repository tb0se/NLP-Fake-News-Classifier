{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "name": "Word Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tb0se/NLP-Fake-News-Classifier/blob/Dev/Word%20Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kADwtIucXxji"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import gensim\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBMleRVgNypb"
      },
      "source": [
        "**Mount drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgewEefXNxZh",
        "outputId": "cbcb9c3f-a7fd-4503-efb2-e8bcb61f7885"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_UpoC4oXxjo"
      },
      "source": [
        "# Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_-sdMgFXxjq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "33cc5d26-ff74-4b63-86c8-629e056ac65f"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Datasets/dataset.csv\")\n",
        "dataset.sample(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>clean_text_without_freq_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Eastern Cape man in court for murder of prosec...</td>\n",
              "      <td>False</td>\n",
              "      <td>eastern cape man court murder prosecutor man a...</td>\n",
              "      <td>eastern cape man court murder prosecutor man a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>Gauteng resident blacklisted for not paying E-...</td>\n",
              "      <td>True</td>\n",
              "      <td>gauteng resident blacklisted paying etoll gaut...</td>\n",
              "      <td>gauteng resident blacklisted paying etoll gaut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>3 suspects in alleged possession of 70 Sassa c...</td>\n",
              "      <td>False</td>\n",
              "      <td>3 suspect alleged possession 70 sassa card cau...</td>\n",
              "      <td>3 suspect alleged possession 70 sassa card cau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>‘Hang your heads in shame’ - DA tells ANC MPs ...</td>\n",
              "      <td>False</td>\n",
              "      <td>hang head shame da tell anc mp refused hold zw...</td>\n",
              "      <td>hang head shame da tell anc mp refused hold zw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>735</th>\n",
              "      <td>Mpumalanga man appears in court after cops fin...</td>\n",
              "      <td>False</td>\n",
              "      <td>mpumalanga man appears court cop find around 9...</td>\n",
              "      <td>mpumalanga man appears court cop find around 9...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  ...                      clean_text_without_freq_words\n",
              "36   Eastern Cape man in court for murder of prosec...  ...  eastern cape man court murder prosecutor man a...\n",
              "672  Gauteng resident blacklisted for not paying E-...  ...  gauteng resident blacklisted paying etoll gaut...\n",
              "366  3 suspects in alleged possession of 70 Sassa c...  ...  3 suspect alleged possession 70 sassa card cau...\n",
              "127  ‘Hang your heads in shame’ - DA tells ANC MPs ...  ...  hang head shame da tell anc mp refused hold zw...\n",
              "735  Mpumalanga man appears in court after cops fin...  ...  mpumalanga man appears court cop find around 9...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnddVrQ5b26B"
      },
      "source": [
        "# Split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b50RE46Gb6HE",
        "outputId": "544e9652-9096-4fb1-dedc-fc771ceb33d3"
      },
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(dataset['clean_text'],dataset['fake'],\n",
        "                                                test_size=0.2, random_state=1)\n",
        "print(f'X: Training set {X_train.shape}, Testing set {X_test.shape}')\n",
        "print(f'y: Training set {y_train.shape}, Testing set {y_test.shape}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: Training set (1168,), Testing set (292,)\n",
            "y: Training set (1168,), Testing set (292,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tt6_BOncgJ1"
      },
      "source": [
        "# 1. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkexKIRQyzOB"
      },
      "source": [
        "## 1.1 Create datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM0DurCPy5v4"
      },
      "source": [
        "# Custom Dataset\n",
        "class NewsDataset(Dataset):\n",
        "  def __init__(self, df_text, df_label):\n",
        "    self.text = df_text\n",
        "    self.label = df_label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return  self.label.iloc[idx],self.text.iloc[idx]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtvglAIZy9hf"
      },
      "source": [
        "train_dataset = NewsDataset(X_train, y_train)\n",
        "test_dataset = NewsDataset(X_test, y_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FzjgK35zCD0"
      },
      "source": [
        "## 1.2 Create Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5cmwYZhzFl8"
      },
      "source": [
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "  for text in data_iter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(X_train.to_list()), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiYQL1MqzvnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02bc7d17-3664-4c94-ca62-f0265c6505b1"
      },
      "source": [
        "words_index = vocab.get_stoi() \n",
        "for i, (word, num) in enumerate(words_index.items()):\n",
        "  print(f'{word} => {num}')\n",
        "  if i == 10:\n",
        "    break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zwide => 26219\n",
            "zuziwe => 26213\n",
            "zungulain => 26212\n",
            "zumazuma => 26211\n",
            "zululand => 26209\n",
            "zuckerburg => 26208\n",
            "zoomed => 26206\n",
            "ziphora => 26201\n",
            "zindzi => 26196\n",
            "zimlive => 26195\n",
            "zimkhitha => 26194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8mWVyUzLfh"
      },
      "source": [
        "## 1.3 Data processing pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEjnWSwPzSYo"
      },
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5U-HvdHzYOS"
      },
      "source": [
        "## 1.4 Create dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo6SPpLwze4E"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(DEVICE), text_list.to(DEVICE), offsets.to(DEVICE)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AQxNvgyzhyN"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=False, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=5, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aD2TzUzcFri"
      },
      "source": [
        "## 1.5 Ensure things make sense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N12HQJVpcML0",
        "outputId": "5e155dc1-0b3a-4b88-c744-0365521bafcb"
      },
      "source": [
        "idx = 0\n",
        "\n",
        "# Text\n",
        "label,text = train_dataset[idx]\n",
        "print(f'from: {text},\\n len:{len(text.split())}')\n",
        "\n",
        "# Tokens\n",
        "labels, tokens,offsets = next(iter(train_dataloader))\n",
        "print(f'to: {tokens[offsets[0].item():offsets[1].item()].cpu().tolist()}')\n",
        "\n",
        "# Check\n",
        "print(f'Check: {text.split()[0]} => idx in vocab: {vocab[text.split()[0]]}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from: expropriate land western cape malema tell supporter eff leader julius malema told resident kayamandi stellenbosch party would expropriate land western cape come power malema kicked party election campaign province thursday 10 day resident expected cast voteskayamandi resident came number rallythe area plagued year problem unemployment lack housingresidents told malema issue faced daily basis saying toilet access residential waste collection service proper housing malema wasted time criticising anc da governance nationally provincially stellenbosch racist town want change scared eff change place better eff want everyone stand together one said malema added running water flushing toilet area lashed anc da supplying adequate toilet housing lived informal settlement animal dont running water flushing toilet da anc treat u like animal need electricity wait housing electricity people need electricity malema said 2016 municipal election da received majority 30 seat councilthese white farm belong u want land take south africa expropriation land start must take land also used opportunity take swipe lobby group political party calling western cape independent state useless people saying want western cape belongs africa belongs everyone end address malema promised female supporter party would buy housewe buy house accommodate said,\n",
            " len:189\n",
            "to: [4472, 172, 124, 11, 385, 418, 1100, 243, 136, 1047, 385, 43, 139, 11434, 1736, 13, 10, 4472, 172, 124, 11, 84, 185, 385, 2366, 13, 115, 321, 178, 176, 245, 30, 139, 133, 2919, 25741, 139, 221, 140, 22759, 116, 5689, 12, 281, 1151, 686, 18698, 43, 385, 204, 1198, 528, 770, 210, 2411, 606, 2853, 1540, 3489, 25, 1528, 839, 385, 2107, 16, 10572, 5, 86, 807, 6401, 22475, 1736, 342, 65, 58, 350, 2027, 243, 350, 118, 467, 243, 58, 547, 680, 431, 8, 1, 385, 71, 602, 114, 10992, 2411, 116, 11520, 5, 86, 7767, 5290, 2411, 839, 1823, 1815, 1410, 901, 111, 602, 114, 10992, 2411, 86, 5, 2105, 55, 19, 901, 70, 463, 1492, 839, 463, 2, 70, 463, 385, 1, 572, 179, 115, 86, 162, 384, 618, 1613, 16344, 21, 145, 4377, 55, 58, 172, 66, 6, 9, 1866, 172, 320, 79, 66, 172, 3, 151, 292, 66, 6630, 4547, 90, 109, 13, 875, 124, 11, 589, 20, 3982, 2, 210, 58, 124, 11, 3276, 9, 3276, 547, 205, 578, 385, 909, 727, 1100, 13, 10, 855, 18694, 855, 53, 2761, 1]\n",
            "Check: expropriate => idx in vocab: 4472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZfi8mTW12_P"
      },
      "source": [
        "## Pretrained embedding model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9L6eP1dj6xT"
      },
      "source": [
        "### 1.1 Create preprocessed corpus(list of lists of n-grams)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWIB3ZCdXxju"
      },
      "source": [
        "# Create a list of lists of unigrams\n",
        "def create_unigrams_list(corpus):\n",
        "  corpus_lst = []\n",
        "\n",
        "  for article in corpus:\n",
        "    words_lst = article.split()\n",
        "    grams_lst = [\" \".join(words_lst[i:i+1]) for i in range(0, len(words_lst), 1)]\n",
        "    corpus_lst.append(grams_lst)\n",
        "  return corpus_lst"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMEeU2mSkRn9"
      },
      "source": [
        "corpus_lst_train = create_unigrams_list(X_train.copy())\n",
        "corpus_lst_test =  create_unigrams_list(X_test.copy())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pLvQMl-Kp"
      },
      "source": [
        "### 1.2 Embedding model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mK7ClZJPZEn"
      },
      "source": [
        "# Embedding model\n",
        "word2vec = gensim.models.word2vec.Word2Vec(corpus_lst_train, size=100, window=5, min_count=1, sg=1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dbRWKSPQ-_o",
        "outputId": "9aed0db6-40bf-42cf-ecfe-3cc05b969749"
      },
      "source": [
        "# Vocab size\n",
        "print('Vocab size:',len(word2vec.wv.vocab))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 26220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgJojLifmMhg",
        "outputId": "68df0806-77a8-4bbf-8472-7bdfbd9550c6"
      },
      "source": [
        "# Example\n",
        "word2vec.wv.most_similar('police')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('station', 0.8610403537750244),\n",
              " ('sap', 0.8550636768341064),\n",
              " ('officer', 0.849295973777771),\n",
              " ('investigating', 0.8382140398025513),\n",
              " ('brigadier', 0.838107705116272),\n",
              " ('suspect', 0.8354823589324951),\n",
              " ('colonel', 0.8322020173072815),\n",
              " ('thembeka', 0.8277255892753601),\n",
              " ('naidu', 0.8248935341835022),\n",
              " ('mbele', 0.8247473239898682)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZVTmzHOf9e5"
      },
      "source": [
        "# Deep Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMrJ3e4doxK6"
      },
      "source": [
        "## 1 Create News classification models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVGDFMx4gTWu"
      },
      "source": [
        "class NewsClassificationModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embed_dim, num_class):\n",
        "    super(NewsClassificationModel, self).__init__()\n",
        "\n",
        "    self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "    self.fc1 = nn.Linear(embed_dim,num_class)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "    self.__init_weights()\n",
        "\n",
        "  def __init_weights(self):\n",
        "    initrange = 0.5\n",
        "    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc1.bias.data.zero_()\n",
        "\n",
        "  def forward(self, text, offsets):\n",
        "    embedded = self.embedding(text, offsets)\n",
        "\n",
        "    # Last linear layers\n",
        "    out = self.fc1(embedded)\n",
        "\n",
        "    return self.sig(out)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ_DjZjZozmy"
      },
      "source": [
        "## 2 Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B17oQcM1o3OC"
      },
      "source": [
        "def train_model(model,dataloader,curr_epoch,criterion,optimizer,print_freq = 2):\n",
        "  model.train()\n",
        "  total_acc, total_count = 0,0\n",
        "\n",
        "  for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    probs = model(text, offsets)\n",
        "\n",
        "    preds = torch.round(probs)\n",
        "    label = torch.unsqueeze(label, 1)\n",
        "    label = label.to(torch.float)\n",
        "\n",
        "    loss = criterion(preds, label)\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "    optimizer.step()\n",
        "\n",
        "    total_acc += (preds == label).sum().item()\n",
        "    total_count += label.size(0)\n",
        "\n",
        "    if idx % print_freq == 0:\n",
        "      print(f'epoch {curr_epoch} | {idx}/{len(dataloader)} batches | accuracy {total_acc/total_count}')\n",
        "      total_acc, total_count = 0,0\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI2ha6Pxs9UK"
      },
      "source": [
        "## 3 Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkzCj0b6s9rY"
      },
      "source": [
        "def eval_model(model, dataloader, criterion):\n",
        "  model.eval()\n",
        "\n",
        "  total_acc, total_count = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "      probs = model(text, offsets)\n",
        "\n",
        "      preds = torch.round(probs)\n",
        "      label = torch.unsqueeze(label, 1)\n",
        "      label = label.to(torch.float)\n",
        "\n",
        "      loss = criterion(preds, label)\n",
        "      total_acc += (preds == label).sum().item()\n",
        "      total_count += label.size(0)\n",
        "\n",
        "  return total_acc/total_count"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URYMy_4wueSC"
      },
      "source": [
        "## 4 Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTtN_okmukrR"
      },
      "source": [
        "def run(model, num_epochs, dataloader, criterion, optimizer, scheduler):\n",
        "  tot_start_time = time.time()\n",
        "  total_acc = None\n",
        "\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    epoch_start_time = time.time()\n",
        "    \n",
        "    # Training\n",
        "    train_model(model, dataloader['train'], epoch, criterion, optimizer )\n",
        "\n",
        "    # Validation\n",
        "    acc_val = eval_model(model, dataloader['valid'], criterion)\n",
        "\n",
        "    if total_acc is not None and total_acc > acc_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "      total_acc = acc_val\n",
        "\n",
        "    print('-'*60)\n",
        "    print(f'end of epoch {epoch} | time:{time.time() - epoch_start_time}s | valid accuracy {acc_val}')\n",
        "    print('-'*60)\n",
        "  \n",
        "  print(f'Elapsed time {time.time() - tot_start_time}')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTimfB-BrXtK"
      },
      "source": [
        "## 5 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyjHlJfdyqnH"
      },
      "source": [
        "### 5.1 Initialise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rqw964rOrZGl",
        "outputId": "acc3d3a2-7c34-4f3b-af94-f3d35416e66e"
      },
      "source": [
        "hyper_params = {\n",
        "    'epochs': 10,\n",
        "    'batch-size': 64,\n",
        "    'learning-rate':5,\n",
        "    'num-classes':1,\n",
        "    'embedding-size': 64\n",
        "}\n",
        "\n",
        "# Model \n",
        "vocab_size = len(vocab)\n",
        "news_model = NewsClassificationModel(vocab_size, hyper_params['embedding-size'], hyper_params['num-classes'] ).to(DEVICE)\n",
        "print(news_model)\n",
        "\n",
        "# criterion\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optimizer \n",
        "optimizer = optim.SGD(news_model.parameters(), lr=hyper_params['learning-rate'])\n",
        "\n",
        "# Scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NewsClassificationModel(\n",
            "  (embedding): EmbeddingBag(26221, 64, mode=mean)\n",
            "  (fc1): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWsViQTTytNC"
      },
      "source": [
        "### 5.2 Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABjCnNl6yxFZ",
        "outputId": "b9c8878c-40db-4d63-ae42-ef8b974662ef"
      },
      "source": [
        "num_train = int(len(train_dataset) * 0.75)\n",
        "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset)-num_train])\n",
        "print(f'Training dataset {len(split_train_)}')\n",
        "print(f'Validation dataset {len(split_valid_)}')\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=hyper_params['batch-size'], shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=hyper_params['batch-size'], shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "dataloader = {\n",
        "    'train': train_dataloader,\n",
        "    'valid': valid_dataloader\n",
        "}"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset 876\n",
            "Validation dataset 292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94wy6RlB1pCQ"
      },
      "source": [
        "### 5.3 Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC4xXJ8O1sFB",
        "outputId": "ac0f461a-a081-43ec-dbfe-4318143863f2"
      },
      "source": [
        "run(news_model, hyper_params['epochs'], dataloader, criterion, optimizer, scheduler)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 | 0/14 batches | accuracy 0.921875\n",
            "epoch 1 | 2/14 batches | accuracy 0.890625\n",
            "epoch 1 | 4/14 batches | accuracy 0.921875\n",
            "epoch 1 | 6/14 batches | accuracy 0.8828125\n",
            "epoch 1 | 8/14 batches | accuracy 0.921875\n",
            "epoch 1 | 10/14 batches | accuracy 0.875\n",
            "epoch 1 | 12/14 batches | accuracy 0.9453125\n",
            "------------------------------------------------------------\n",
            "end of epoch 1 | time:0.3687865734100342s | valid accuracy 0.8013698630136986\n",
            "------------------------------------------------------------\n",
            "epoch 2 | 0/14 batches | accuracy 0.90625\n",
            "epoch 2 | 2/14 batches | accuracy 0.890625\n",
            "epoch 2 | 4/14 batches | accuracy 0.921875\n",
            "epoch 2 | 6/14 batches | accuracy 0.90625\n",
            "epoch 2 | 8/14 batches | accuracy 0.9375\n",
            "epoch 2 | 10/14 batches | accuracy 0.8984375\n",
            "epoch 2 | 12/14 batches | accuracy 0.921875\n",
            "------------------------------------------------------------\n",
            "end of epoch 2 | time:0.3392493724822998s | valid accuracy 0.8013698630136986\n",
            "------------------------------------------------------------\n",
            "epoch 3 | 0/14 batches | accuracy 0.890625\n",
            "epoch 3 | 2/14 batches | accuracy 0.90625\n",
            "epoch 3 | 4/14 batches | accuracy 0.921875\n",
            "epoch 3 | 6/14 batches | accuracy 0.9296875\n",
            "epoch 3 | 8/14 batches | accuracy 0.890625\n",
            "epoch 3 | 10/14 batches | accuracy 0.8671875\n",
            "epoch 3 | 12/14 batches | accuracy 0.9375\n",
            "------------------------------------------------------------\n",
            "end of epoch 3 | time:0.332275390625s | valid accuracy 0.8013698630136986\n",
            "------------------------------------------------------------\n",
            "epoch 4 | 0/14 batches | accuracy 0.953125\n",
            "epoch 4 | 2/14 batches | accuracy 0.921875\n",
            "epoch 4 | 4/14 batches | accuracy 0.921875\n",
            "epoch 4 | 6/14 batches | accuracy 0.9140625\n",
            "epoch 4 | 8/14 batches | accuracy 0.9140625\n",
            "epoch 4 | 10/14 batches | accuracy 0.875\n",
            "epoch 4 | 12/14 batches | accuracy 0.8984375\n",
            "------------------------------------------------------------\n",
            "end of epoch 4 | time:0.35019898414611816s | valid accuracy 0.8013698630136986\n",
            "------------------------------------------------------------\n",
            "epoch 5 | 0/14 batches | accuracy 0.859375\n",
            "epoch 5 | 2/14 batches | accuracy 0.953125\n",
            "epoch 5 | 4/14 batches | accuracy 0.921875\n",
            "epoch 5 | 6/14 batches | accuracy 0.8984375\n",
            "epoch 5 | 8/14 batches | accuracy 0.9375\n",
            "epoch 5 | 10/14 batches | accuracy 0.8984375\n",
            "epoch 5 | 12/14 batches | accuracy 0.9140625\n",
            "------------------------------------------------------------\n",
            "end of epoch 5 | time:0.3287508487701416s | valid accuracy 0.8013698630136986\n",
            "------------------------------------------------------------\n",
            "epoch 6 | 0/14 batches | accuracy 0.875\n",
            "epoch 6 | 2/14 batches | accuracy 0.90625\n",
            "epoch 6 | 4/14 batches | accuracy 0.921875\n",
            "epoch 6 | 6/14 batches | accuracy 0.9609375\n",
            "epoch 6 | 8/14 batches | accuracy 0.9140625\n",
            "epoch 6 | 10/14 batches | accuracy 0.8828125\n",
            "epoch 6 | 12/14 batches | accuracy 0.8984375\n",
            "------------------------------------------------------------\n",
            "end of epoch 6 | time:0.35112571716308594s | valid accuracy 0.8013698630136986\n",
            "------------------------------------------------------------\n",
            "epoch 7 | 0/14 batches | accuracy 0.96875\n",
            "epoch 7 | 2/14 batches | accuracy 0.8828125\n",
            "epoch 7 | 4/14 batches | accuracy 0.921875\n",
            "epoch 7 | 6/14 batches | accuracy 0.859375\n",
            "epoch 7 | 8/14 batches | accuracy 0.921875\n",
            "epoch 7 | 10/14 batches | accuracy 0.9375\n",
            "epoch 7 | 12/14 batches | accuracy 0.90625\n",
            "------------------------------------------------------------\n",
            "end of epoch 7 | time:0.34025073051452637s | valid accuracy 0.8013698630136986\n",
            "------------------------------------------------------------\n",
            "epoch 8 | 0/14 batches | accuracy 0.921875\n",
            "epoch 8 | 2/14 batches | accuracy 0.8515625\n",
            "epoch 8 | 4/14 batches | accuracy 0.8828125\n",
            "epoch 8 | 6/14 batches | accuracy 0.9453125\n",
            "epoch 8 | 8/14 batches | accuracy 0.9375\n",
            "epoch 8 | 10/14 batches | accuracy 0.9296875\n",
            "epoch 8 | 12/14 batches | accuracy 0.90625\n",
            "------------------------------------------------------------\n",
            "end of epoch 8 | time:0.33054208755493164s | valid accuracy 0.8013698630136986\n",
            "------------------------------------------------------------\n",
            "epoch 9 | 0/14 batches | accuracy 0.90625\n",
            "epoch 9 | 2/14 batches | accuracy 0.890625\n",
            "epoch 9 | 4/14 batches | accuracy 0.90625\n",
            "epoch 9 | 6/14 batches | accuracy 0.90625\n",
            "epoch 9 | 8/14 batches | accuracy 0.9453125\n",
            "epoch 9 | 10/14 batches | accuracy 0.8984375\n",
            "epoch 9 | 12/14 batches | accuracy 0.9296875\n",
            "------------------------------------------------------------\n",
            "end of epoch 9 | time:0.34529566764831543s | valid accuracy 0.8013698630136986\n",
            "------------------------------------------------------------\n",
            "epoch 10 | 0/14 batches | accuracy 0.890625\n",
            "epoch 10 | 2/14 batches | accuracy 0.9140625\n",
            "epoch 10 | 4/14 batches | accuracy 0.859375\n",
            "epoch 10 | 6/14 batches | accuracy 0.9296875\n",
            "epoch 10 | 8/14 batches | accuracy 0.9453125\n",
            "epoch 10 | 10/14 batches | accuracy 0.921875\n",
            "epoch 10 | 12/14 batches | accuracy 0.921875\n",
            "------------------------------------------------------------\n",
            "end of epoch 10 | time:0.3337557315826416s | valid accuracy 0.8013698630136986\n",
            "------------------------------------------------------------\n",
            "Elapsed time 3.424668312072754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxF_K4fR2AB6"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}