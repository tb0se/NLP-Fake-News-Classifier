{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Word_Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kADwtIucXxji"
      },
      "source": [
        "import time\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import gensim\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBMleRVgNypb"
      },
      "source": [
        "**Mount drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgewEefXNxZh",
        "outputId": "4c81df7e-4796-48da-d351-9016030bc64e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_UpoC4oXxjo"
      },
      "source": [
        "# Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "k_-sdMgFXxjq",
        "outputId": "cf05f5e3-8fb5-4d99-9bfa-4fb890ee2c1e"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Datasets/dataset.csv\")\n",
        "dataset.sample(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>clean_text_without_freq_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1363</th>\n",
              "      <td>VIDEO: Whole Troop of EMPD Officers Abuse &amp; Th...</td>\n",
              "      <td>True</td>\n",
              "      <td>video whole troop empd officer abuse threaten ...</td>\n",
              "      <td>video whole troop empd officer abuse threaten ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>Ramaphosa, ANC &amp; Abdool Karim Want To Flatten ...</td>\n",
              "      <td>True</td>\n",
              "      <td>ramaphosa anc abdool karim want flatten econom...</td>\n",
              "      <td>ramaphosa anc abdool karim want flatten econom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>938</th>\n",
              "      <td>Sanef condemns robbery of Newzroom Afrika crew...</td>\n",
              "      <td>False</td>\n",
              "      <td>sanef condemns robbery newzroom afrika crew ea...</td>\n",
              "      <td>sanef condemns robbery newzroom afrika crew ea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>868</th>\n",
              "      <td>Former Chiefs midfielder Lucky Maselesele murd...</td>\n",
              "      <td>False</td>\n",
              "      <td>former chief midfielder lucky maselesele murde...</td>\n",
              "      <td>former chief midfielder lucky maselesele murde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1254</th>\n",
              "      <td>'I lent him R5 000' - alleged victim of rape-a...</td>\n",
              "      <td>False</td>\n",
              "      <td>lent r5 000 alleged victim rapeaccused mpumala...</td>\n",
              "      <td>lent r5 000 alleged victim rapeaccused mpumala...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  ...                      clean_text_without_freq_words\n",
              "1363  VIDEO: Whole Troop of EMPD Officers Abuse & Th...  ...  video whole troop empd officer abuse threaten ...\n",
              "200   Ramaphosa, ANC & Abdool Karim Want To Flatten ...  ...  ramaphosa anc abdool karim want flatten econom...\n",
              "938   Sanef condemns robbery of Newzroom Afrika crew...  ...  sanef condemns robbery newzroom afrika crew ea...\n",
              "868   Former Chiefs midfielder Lucky Maselesele murd...  ...  former chief midfielder lucky maselesele murde...\n",
              "1254  'I lent him R5 000' - alleged victim of rape-a...  ...  lent r5 000 alleged victim rapeaccused mpumala...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnddVrQ5b26B"
      },
      "source": [
        "# Split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b50RE46Gb6HE",
        "outputId": "d33754cc-ceb2-4f46-ae87-27bcb1dadb3b"
      },
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(dataset['clean_text'],dataset['fake'],\n",
        "                                                test_size=0.2, random_state=1)\n",
        "print(f'X: Training set {X_train.shape}, Testing set {X_test.shape}')\n",
        "print(f'y: Training set {y_train.shape}, Testing set {y_test.shape}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: Training set (1168,), Testing set (292,)\n",
            "y: Training set (1168,), Testing set (292,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sjoTwAMjp29"
      },
      "source": [
        "#Vectorization: Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKE3EpRvj5J1"
      },
      "source": [
        "## 1 Create preprocessed corpus(list of lists of n-grams)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JASS9R9gjS9_"
      },
      "source": [
        "# Create a list of lists of unigrams\n",
        "def create_unigrams_list(corpus):\n",
        "  corpus_lst = []\n",
        "\n",
        "  for article in corpus:\n",
        "    words_lst = article.split()\n",
        "    grams_lst = [\" \".join(words_lst[i:i+1]) for i in range(0, len(words_lst), 1)]\n",
        "    corpus_lst.append(grams_lst)\n",
        "  return corpus_lst"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gObc71MYjS3V"
      },
      "source": [
        "corpus_lst_train = create_unigrams_list(X_train.copy())\n",
        "corpus_lst_test =  create_unigrams_list(X_test.copy())"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEazEYy-jzwh"
      },
      "source": [
        "## 2 Embedding model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytx8VlrTjSxz"
      },
      "source": [
        "# Embedding model\n",
        "word2vec = gensim.models.word2vec.Word2Vec(corpus_lst_train, size=100, window=5, min_count=1, sg=1)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk5Ty7ThkE-6"
      },
      "source": [
        "## 3 Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkuRnJV4jSoc",
        "outputId": "bd343e64-853e-4025-ee9f-0450efbd9e7b"
      },
      "source": [
        "# Vocab size\n",
        "print('Vocab size:',len(word2vec.wv.vocab))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 26220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npmY3rWGjScq",
        "outputId": "15264ae4-98ad-4ea5-ad04-35b506f41ce9"
      },
      "source": [
        "# Example\n",
        "word2vec.wv.most_similar('police')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sap', 0.8775656223297119),\n",
              " ('station', 0.863582968711853),\n",
              " ('officer', 0.8578199148178101),\n",
              " ('brigadier', 0.8316795229911804),\n",
              " ('colonel', 0.8313194513320923),\n",
              " ('mbele', 0.8295994997024536),\n",
              " ('investigating', 0.8286892771720886),\n",
              " ('thembeka', 0.824602484703064),\n",
              " ('naidu', 0.8243770599365234),\n",
              " ('naidoo', 0.8192735314369202)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tt6_BOncgJ1"
      },
      "source": [
        "# 1. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkexKIRQyzOB"
      },
      "source": [
        "## 1.1 Create datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM0DurCPy5v4"
      },
      "source": [
        "# Custom Dataset\n",
        "class NewsDataset(Dataset):\n",
        "  def __init__(self, df_text, df_label):\n",
        "    self.text = df_text\n",
        "    self.label = df_label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return  self.label.iloc[idx],self.text.iloc[idx]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtvglAIZy9hf"
      },
      "source": [
        "train_dataset = NewsDataset(X_train, y_train)\n",
        "test_dataset = NewsDataset(X_test, y_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FzjgK35zCD0"
      },
      "source": [
        "## 1.2 Create Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5cmwYZhzFl8"
      },
      "source": [
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "  for text in data_iter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(X_train.to_list()), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiYQL1MqzvnA",
        "outputId": "374403a5-5474-405b-92b9-5fda1dedec50"
      },
      "source": [
        "words_index = vocab.get_stoi() \n",
        "for i, (word, num) in enumerate(words_index.items()):\n",
        "  print(f'{word} => {num}')\n",
        "  if i == 10:\n",
        "    break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zwide => 26219\n",
            "zuziwe => 26213\n",
            "zungulain => 26212\n",
            "zumazuma => 26211\n",
            "zululand => 26209\n",
            "zuckerburg => 26208\n",
            "zoomed => 26206\n",
            "ziphora => 26201\n",
            "zindzi => 26196\n",
            "zimlive => 26195\n",
            "zimkhitha => 26194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8mWVyUzLfh"
      },
      "source": [
        "## 1.3 Data processing pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEjnWSwPzSYo"
      },
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5U-HvdHzYOS"
      },
      "source": [
        "## 1.4 Create dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo6SPpLwze4E"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(DEVICE), text_list.to(DEVICE), offsets.to(DEVICE)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AQxNvgyzhyN"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=False, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=5, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aD2TzUzcFri"
      },
      "source": [
        "## 1.5 Ensure things make sense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N12HQJVpcML0",
        "outputId": "65efc420-547b-479f-d60f-8d52f57c845c"
      },
      "source": [
        "idx = 0\n",
        "\n",
        "# Text\n",
        "label,text = train_dataset[idx]\n",
        "print(f'from: {text},\\n len:{len(text.split())}')\n",
        "\n",
        "# Tokens\n",
        "labels, tokens,offsets = next(iter(train_dataloader))\n",
        "print(f'to: {tokens[offsets[0].item():offsets[1].item()].cpu().tolist()}')\n",
        "\n",
        "# Check\n",
        "print(f'Check: {text.split()[0]} => idx in vocab: {vocab[text.split()[0]]}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from: expropriate land western cape malema tell supporter eff leader julius malema told resident kayamandi stellenbosch party would expropriate land western cape come power malema kicked party election campaign province thursday 10 day resident expected cast voteskayamandi resident came number rallythe area plagued year problem unemployment lack housingresidents told malema issue faced daily basis saying toilet access residential waste collection service proper housing malema wasted time criticising anc da governance nationally provincially stellenbosch racist town want change scared eff change place better eff want everyone stand together one said malema added running water flushing toilet area lashed anc da supplying adequate toilet housing lived informal settlement animal dont running water flushing toilet da anc treat u like animal need electricity wait housing electricity people need electricity malema said 2016 municipal election da received majority 30 seat councilthese white farm belong u want land take south africa expropriation land start must take land also used opportunity take swipe lobby group political party calling western cape independent state useless people saying want western cape belongs africa belongs everyone end address malema promised female supporter party would buy housewe buy house accommodate said,\n",
            " len:189\n",
            "to: [4472, 172, 124, 11, 385, 418, 1100, 243, 136, 1047, 385, 43, 139, 11434, 1736, 13, 10, 4472, 172, 124, 11, 84, 185, 385, 2366, 13, 115, 321, 178, 176, 245, 30, 139, 133, 2919, 25741, 139, 221, 140, 22759, 116, 5689, 12, 281, 1151, 686, 18698, 43, 385, 204, 1198, 528, 770, 210, 2411, 606, 2853, 1540, 3489, 25, 1528, 839, 385, 2107, 16, 10572, 5, 86, 807, 6401, 22475, 1736, 342, 65, 58, 350, 2027, 243, 350, 118, 467, 243, 58, 547, 680, 431, 8, 1, 385, 71, 602, 114, 10992, 2411, 116, 11520, 5, 86, 7767, 5290, 2411, 839, 1823, 1815, 1410, 901, 111, 602, 114, 10992, 2411, 86, 5, 2105, 55, 19, 901, 70, 463, 1492, 839, 463, 2, 70, 463, 385, 1, 572, 179, 115, 86, 162, 384, 618, 1613, 16344, 21, 145, 4377, 55, 58, 172, 66, 6, 9, 1866, 172, 320, 79, 66, 172, 3, 151, 292, 66, 6630, 4547, 90, 109, 13, 875, 124, 11, 589, 20, 3982, 2, 210, 58, 124, 11, 3276, 9, 3276, 547, 205, 578, 385, 909, 727, 1100, 13, 10, 855, 18694, 855, 53, 2761, 1]\n",
            "Check: expropriate => idx in vocab: 4472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZVTmzHOf9e5"
      },
      "source": [
        "# Deep Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMrJ3e4doxK6"
      },
      "source": [
        "## 1 Create News classification models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVGDFMx4gTWu"
      },
      "source": [
        "class NewsClassificationModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embed_dim, num_class,weight):\n",
        "    super(NewsClassificationModel, self).__init__()\n",
        "\n",
        "    # self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "    self.embedding_bag = nn.EmbeddingBag.from_pretrained(weight, freeze=True)\n",
        "    self.fc1 = nn.Linear(embed_dim,32)\n",
        "    self.fc2 = nn.Linear(32,num_class)\n",
        "    self.sig = nn.Sigmoid()\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.__init_weights()\n",
        "\n",
        "  def __init_weights(self):\n",
        "    initrange = 0.5\n",
        "    # self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc1.bias.data.zero_()\n",
        "    self.fc2.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc2.bias.data.zero_()\n",
        "\n",
        "  def forward(self, text, offsets):\n",
        "    # embedded = self.embedding(text, offsets)\n",
        "    embedded = self.embedding_bag(text, offsets)\n",
        "\n",
        "    # Last linear layers\n",
        "    out = self.fc1(embedded)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "\n",
        "    return self.sig(out)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ_DjZjZozmy"
      },
      "source": [
        "## 2 Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B17oQcM1o3OC"
      },
      "source": [
        "def train_model(model,dataloader,curr_epoch,criterion,optimizer,print_freq = 2):\n",
        "  model.train()\n",
        "  total_acc, total_count = 0,0\n",
        "\n",
        "  for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    probs = model(text, offsets)\n",
        "\n",
        "    preds = torch.round(probs)\n",
        "    label = torch.unsqueeze(label, 1)\n",
        "    label = label.to(torch.float)\n",
        "\n",
        "    loss = criterion(preds, label)\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "    optimizer.step()\n",
        "\n",
        "    total_acc += (preds == label).sum().item()\n",
        "    total_count += label.size(0)\n",
        "\n",
        "    if idx % print_freq == 0:\n",
        "      print(f'epoch {curr_epoch} | {idx}/{len(dataloader)} batches | accuracy {total_acc/total_count}')\n",
        "      total_acc, total_count = 0,0\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI2ha6Pxs9UK"
      },
      "source": [
        "## 3 Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkzCj0b6s9rY"
      },
      "source": [
        "def eval_model(model, dataloader, criterion):\n",
        "  model.eval()\n",
        "\n",
        "  total_acc, total_count = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "      probs = model(text, offsets)\n",
        "\n",
        "      preds = torch.round(probs)\n",
        "      label = torch.unsqueeze(label, 1)\n",
        "      label = label.to(torch.float)\n",
        "\n",
        "      loss = criterion(preds, label)\n",
        "      total_acc += (preds == label).sum().item()\n",
        "      total_count += label.size(0)\n",
        "\n",
        "  return total_acc/total_count"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URYMy_4wueSC"
      },
      "source": [
        "## 4 Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTtN_okmukrR"
      },
      "source": [
        "def run(model, num_epochs, dataloader, criterion, optimizer, scheduler):\n",
        "  tot_start_time = time.time()\n",
        "  total_acc = None\n",
        "\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    epoch_start_time = time.time()\n",
        "    \n",
        "    # Training\n",
        "    train_model(model, dataloader['train'], epoch, criterion, optimizer )\n",
        "\n",
        "    # Validation\n",
        "    acc_val = eval_model(model, dataloader['valid'], criterion)\n",
        "\n",
        "    if total_acc is not None and total_acc > acc_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "      total_acc = acc_val\n",
        "\n",
        "    print('-'*60)\n",
        "    print(f'end of epoch {epoch} | time:{time.time() - epoch_start_time}s | valid accuracy {acc_val}')\n",
        "    print('-'*60)\n",
        "  \n",
        "  print(f'Elapsed time {time.time() - tot_start_time}')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTimfB-BrXtK"
      },
      "source": [
        "## 5 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyjHlJfdyqnH"
      },
      "source": [
        "### 5.1 Initialise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rqw964rOrZGl",
        "outputId": "3d143044-18e5-4c38-a662-d39662e4ac20"
      },
      "source": [
        "hyper_params = {\n",
        "    'epochs': 15,\n",
        "    'batch-size': 32,\n",
        "    'learning-rate':5,\n",
        "    'num-classes':1,\n",
        "    'embedding-size': 100\n",
        "}\n",
        "\n",
        "# word2vec weight matrix\n",
        "word2vec_weights = torch.FloatTensor(word2vec.wv.vectors)\n",
        "\n",
        "# Model \n",
        "vocab_size = len(vocab)\n",
        "news_model = NewsClassificationModel(vocab_size, hyper_params['embedding-size'], hyper_params['num-classes'], word2vec_weights ).to(DEVICE)\n",
        "print(news_model)\n",
        "\n",
        "# criterion\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optimizer \n",
        "optimizer = optim.SGD(news_model.parameters(), lr=hyper_params['learning-rate'])\n",
        "\n",
        "# Scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NewsClassificationModel(\n",
            "  (embedding_bag): EmbeddingBag(26220, 100, mode=mean)\n",
            "  (fc1): Linear(in_features=100, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWsViQTTytNC"
      },
      "source": [
        "### 5.2 Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABjCnNl6yxFZ",
        "outputId": "b5c8a8af-e0b1-4071-ad56-5c7a1a3474ea"
      },
      "source": [
        "num_train = int(len(train_dataset) * 0.75)\n",
        "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset)-num_train])\n",
        "print(f'Training dataset {len(split_train_)}')\n",
        "print(f'Validation dataset {len(split_valid_)}')\n",
        "print(f'Testing dataset {len(test_dataset)}')\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=hyper_params['batch-size'], shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=hyper_params['batch-size'], shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=hyper_params['batch-size'], shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "dataloader = {\n",
        "    'train': train_dataloader,\n",
        "    'valid': valid_dataloader\n",
        "}"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset 876\n",
            "Validation dataset 292\n",
            "Testing dataset 292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94wy6RlB1pCQ"
      },
      "source": [
        "### 5.3 Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC4xXJ8O1sFB",
        "outputId": "18fed79c-3d36-4f75-cfd2-d27bd00879f2"
      },
      "source": [
        "run(news_model, hyper_params['epochs'], dataloader, criterion, optimizer, scheduler)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 | 0/14 batches | accuracy 0.484375\n",
            "epoch 1 | 2/14 batches | accuracy 0.4921875\n",
            "epoch 1 | 4/14 batches | accuracy 0.4453125\n",
            "epoch 1 | 6/14 batches | accuracy 0.4375\n",
            "epoch 1 | 8/14 batches | accuracy 0.515625\n",
            "epoch 1 | 10/14 batches | accuracy 0.53125\n",
            "epoch 1 | 12/14 batches | accuracy 0.5\n",
            "------------------------------------------------------------\n",
            "end of epoch 1 | time:0.2717909812927246s | valid accuracy 0.4965753424657534\n",
            "------------------------------------------------------------\n",
            "epoch 2 | 0/14 batches | accuracy 0.53125\n",
            "epoch 2 | 2/14 batches | accuracy 0.515625\n",
            "epoch 2 | 4/14 batches | accuracy 0.5234375\n",
            "epoch 2 | 6/14 batches | accuracy 0.5\n",
            "epoch 2 | 8/14 batches | accuracy 0.4609375\n",
            "epoch 2 | 10/14 batches | accuracy 0.5234375\n",
            "epoch 2 | 12/14 batches | accuracy 0.4140625\n",
            "------------------------------------------------------------\n",
            "end of epoch 2 | time:0.2524247169494629s | valid accuracy 0.4965753424657534\n",
            "------------------------------------------------------------\n",
            "epoch 3 | 0/14 batches | accuracy 0.421875\n",
            "epoch 3 | 2/14 batches | accuracy 0.4453125\n",
            "epoch 3 | 4/14 batches | accuracy 0.3671875\n",
            "epoch 3 | 6/14 batches | accuracy 0.4921875\n",
            "epoch 3 | 8/14 batches | accuracy 0.6015625\n",
            "epoch 3 | 10/14 batches | accuracy 0.5390625\n",
            "epoch 3 | 12/14 batches | accuracy 0.578125\n",
            "------------------------------------------------------------\n",
            "end of epoch 3 | time:0.2541015148162842s | valid accuracy 0.4965753424657534\n",
            "------------------------------------------------------------\n",
            "epoch 4 | 0/14 batches | accuracy 0.453125\n",
            "epoch 4 | 2/14 batches | accuracy 0.5\n",
            "epoch 4 | 4/14 batches | accuracy 0.4609375\n",
            "epoch 4 | 6/14 batches | accuracy 0.5859375\n",
            "epoch 4 | 8/14 batches | accuracy 0.4375\n",
            "epoch 4 | 10/14 batches | accuracy 0.5234375\n",
            "epoch 4 | 12/14 batches | accuracy 0.5078125\n",
            "------------------------------------------------------------\n",
            "end of epoch 4 | time:0.2519831657409668s | valid accuracy 0.4965753424657534\n",
            "------------------------------------------------------------\n",
            "epoch 5 | 0/14 batches | accuracy 0.46875\n",
            "epoch 5 | 2/14 batches | accuracy 0.4765625\n",
            "epoch 5 | 4/14 batches | accuracy 0.5234375\n",
            "epoch 5 | 6/14 batches | accuracy 0.5\n",
            "epoch 5 | 8/14 batches | accuracy 0.5859375\n",
            "epoch 5 | 10/14 batches | accuracy 0.453125\n",
            "epoch 5 | 12/14 batches | accuracy 0.4609375\n",
            "------------------------------------------------------------\n",
            "end of epoch 5 | time:0.2550544738769531s | valid accuracy 0.4965753424657534\n",
            "------------------------------------------------------------\n",
            "epoch 6 | 0/14 batches | accuracy 0.484375\n",
            "epoch 6 | 2/14 batches | accuracy 0.4765625\n",
            "epoch 6 | 4/14 batches | accuracy 0.484375\n",
            "epoch 6 | 6/14 batches | accuracy 0.484375\n",
            "epoch 6 | 8/14 batches | accuracy 0.546875\n",
            "epoch 6 | 10/14 batches | accuracy 0.4765625\n",
            "epoch 6 | 12/14 batches | accuracy 0.4921875\n",
            "------------------------------------------------------------\n",
            "end of epoch 6 | time:0.25733447074890137s | valid accuracy 0.4965753424657534\n",
            "------------------------------------------------------------\n",
            "epoch 7 | 0/14 batches | accuracy 0.515625\n",
            "epoch 7 | 2/14 batches | accuracy 0.5234375\n",
            "epoch 7 | 4/14 batches | accuracy 0.484375\n",
            "epoch 7 | 6/14 batches | accuracy 0.4609375\n",
            "epoch 7 | 8/14 batches | accuracy 0.515625\n",
            "epoch 7 | 10/14 batches | accuracy 0.484375\n",
            "epoch 7 | 12/14 batches | accuracy 0.5234375\n",
            "------------------------------------------------------------\n",
            "end of epoch 7 | time:0.24488520622253418s | valid accuracy 0.4965753424657534\n",
            "------------------------------------------------------------\n",
            "epoch 8 | 0/14 batches | accuracy 0.609375\n",
            "epoch 8 | 2/14 batches | accuracy 0.4296875\n",
            "epoch 8 | 4/14 batches | accuracy 0.4765625\n",
            "epoch 8 | 6/14 batches | accuracy 0.5078125\n",
            "epoch 8 | 8/14 batches | accuracy 0.4453125\n",
            "epoch 8 | 10/14 batches | accuracy 0.5\n",
            "epoch 8 | 12/14 batches | accuracy 0.53125\n",
            "------------------------------------------------------------\n",
            "end of epoch 8 | time:0.258669376373291s | valid accuracy 0.4965753424657534\n",
            "------------------------------------------------------------\n",
            "epoch 9 | 0/14 batches | accuracy 0.4375\n",
            "epoch 9 | 2/14 batches | accuracy 0.3984375\n",
            "epoch 9 | 4/14 batches | accuracy 0.53125\n",
            "epoch 9 | 6/14 batches | accuracy 0.484375\n",
            "epoch 9 | 8/14 batches | accuracy 0.59375\n",
            "epoch 9 | 10/14 batches | accuracy 0.4296875\n",
            "epoch 9 | 12/14 batches | accuracy 0.515625\n",
            "------------------------------------------------------------\n",
            "end of epoch 9 | time:0.24967718124389648s | valid accuracy 0.4965753424657534\n",
            "------------------------------------------------------------\n",
            "epoch 10 | 0/14 batches | accuracy 0.46875\n",
            "epoch 10 | 2/14 batches | accuracy 0.46875\n",
            "epoch 10 | 4/14 batches | accuracy 0.6015625\n",
            "epoch 10 | 6/14 batches | accuracy 0.375\n",
            "epoch 10 | 8/14 batches | accuracy 0.515625\n",
            "epoch 10 | 10/14 batches | accuracy 0.5078125\n",
            "epoch 10 | 12/14 batches | accuracy 0.515625\n",
            "------------------------------------------------------------\n",
            "end of epoch 10 | time:0.2466135025024414s | valid accuracy 0.4965753424657534\n",
            "------------------------------------------------------------\n",
            "Elapsed time 2.544968843460083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tkZB_m1yB3v"
      },
      "source": [
        "## 6 Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxF_K4fR2AB6",
        "outputId": "5b8a96c8-97c3-4a82-a0e3-6a1cceae2068"
      },
      "source": [
        "print('Check results of the test dataset')\n",
        "acc_test = eval_model(news_model,test_dataloader, criterion)\n",
        "print(f'Test accuracy {acc_test}')"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check results of the test dataset\n",
            "Test accuracy 0.4863013698630137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5HG5joo8ND7"
      },
      "source": [
        "# Deep Learning Model(pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKRjT8JSoaep"
      },
      "source": [
        "## 1 Pretrained Word2Vec vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwylZUoSojQ_"
      },
      "source": [
        "from gensim.test.utils import datapath\n",
        "\n",
        "pretrained_word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Datasets/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4Il_DRn0xzp"
      },
      "source": [
        "## 2 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvj7BtDD01Ou"
      },
      "source": [
        "### 2.1 Initialise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuDMWJsZqkqj",
        "outputId": "6dc69144-e62a-4103-f80b-902e5763aa3f"
      },
      "source": [
        "pre_hyper_params = {\n",
        "    'epochs': 10,\n",
        "    'batch-size': 32,\n",
        "    'learning-rate':5,\n",
        "    'num-classes':1,\n",
        "    'embedding-size': 300\n",
        "}\n",
        "\n",
        "# Vocabulary size\n",
        "pre_vocab_size = len(pretrained_word2vec_model.vocab)\n",
        "\n",
        "# word2vec weight matrix\n",
        "pre_word2vec_weights = torch.FloatTensor(pretrained_word2vec_model.vectors)\n",
        "\n",
        "# Classifier Model\n",
        "pre_news_model = NewsClassificationModel(pre_vocab_size, pre_hyper_params['embedding-size'], pre_hyper_params['num-classes'], pre_word2vec_weights ).to(DEVICE)\n",
        "print(pre_news_model)\n",
        "\n",
        "# Optimizer\n",
        "pre_optimizer = optim.SGD(pre_news_model.parameters(), lr=pre_hyper_params['learning-rate'])\n",
        "\n",
        "# Scheduler\n",
        "pre_scheduler = optim.lr_scheduler.StepLR(pre_optimizer, 1.0, gamma=0.1)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NewsClassificationModel(\n",
            "  (embedding_bag): EmbeddingBag(3000000, 300, mode=mean)\n",
            "  (fc1): Linear(in_features=300, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV7hXAN437tR"
      },
      "source": [
        "### 2.2 Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeBq7BU335Gw"
      },
      "source": [
        "train_dataloader = DataLoader(split_train_, batch_size=pre_hyper_params['batch-size'], shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=pre_hyper_params['batch-size'], shuffle=True, collate_fn=collate_batch)\n",
        "pre_test_dataloader = DataLoader(test_dataset, batch_size=pre_hyper_params['batch-size'], shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "pre_dataloader = {\n",
        "    'train': train_dataloader,\n",
        "    'valid': valid_dataloader\n",
        "}"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpw_XL372GfA"
      },
      "source": [
        "### 2.3 Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOoA1qyv0-c_",
        "outputId": "553c9fd6-26d3-4ca2-8aa1-155c78240f01"
      },
      "source": [
        "run(pre_news_model, hyper_params['epochs'], pre_dataloader, criterion, pre_optimizer, pre_scheduler)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 | 0/28 batches | accuracy 0.4375\n",
            "epoch 1 | 2/28 batches | accuracy 0.390625\n",
            "epoch 1 | 4/28 batches | accuracy 0.5\n",
            "epoch 1 | 6/28 batches | accuracy 0.546875\n",
            "epoch 1 | 8/28 batches | accuracy 0.5\n",
            "epoch 1 | 10/28 batches | accuracy 0.359375\n",
            "epoch 1 | 12/28 batches | accuracy 0.5625\n",
            "epoch 1 | 14/28 batches | accuracy 0.46875\n",
            "epoch 1 | 16/28 batches | accuracy 0.390625\n",
            "epoch 1 | 18/28 batches | accuracy 0.53125\n",
            "epoch 1 | 20/28 batches | accuracy 0.453125\n",
            "epoch 1 | 22/28 batches | accuracy 0.46875\n",
            "epoch 1 | 24/28 batches | accuracy 0.4375\n",
            "epoch 1 | 26/28 batches | accuracy 0.53125\n",
            "------------------------------------------------------------\n",
            "end of epoch 1 | time:0.29987168312072754s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 2 | 0/28 batches | accuracy 0.4375\n",
            "epoch 2 | 2/28 batches | accuracy 0.515625\n",
            "epoch 2 | 4/28 batches | accuracy 0.46875\n",
            "epoch 2 | 6/28 batches | accuracy 0.53125\n",
            "epoch 2 | 8/28 batches | accuracy 0.484375\n",
            "epoch 2 | 10/28 batches | accuracy 0.46875\n",
            "epoch 2 | 12/28 batches | accuracy 0.453125\n",
            "epoch 2 | 14/28 batches | accuracy 0.4375\n",
            "epoch 2 | 16/28 batches | accuracy 0.390625\n",
            "epoch 2 | 18/28 batches | accuracy 0.46875\n",
            "epoch 2 | 20/28 batches | accuracy 0.46875\n",
            "epoch 2 | 22/28 batches | accuracy 0.484375\n",
            "epoch 2 | 24/28 batches | accuracy 0.453125\n",
            "epoch 2 | 26/28 batches | accuracy 0.46875\n",
            "------------------------------------------------------------\n",
            "end of epoch 2 | time:0.29257965087890625s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 3 | 0/28 batches | accuracy 0.46875\n",
            "epoch 3 | 2/28 batches | accuracy 0.359375\n",
            "epoch 3 | 4/28 batches | accuracy 0.484375\n",
            "epoch 3 | 6/28 batches | accuracy 0.453125\n",
            "epoch 3 | 8/28 batches | accuracy 0.53125\n",
            "epoch 3 | 10/28 batches | accuracy 0.484375\n",
            "epoch 3 | 12/28 batches | accuracy 0.515625\n",
            "epoch 3 | 14/28 batches | accuracy 0.421875\n",
            "epoch 3 | 16/28 batches | accuracy 0.40625\n",
            "epoch 3 | 18/28 batches | accuracy 0.515625\n",
            "epoch 3 | 20/28 batches | accuracy 0.484375\n",
            "epoch 3 | 22/28 batches | accuracy 0.40625\n",
            "epoch 3 | 24/28 batches | accuracy 0.546875\n",
            "epoch 3 | 26/28 batches | accuracy 0.453125\n",
            "------------------------------------------------------------\n",
            "end of epoch 3 | time:0.29955434799194336s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 4 | 0/28 batches | accuracy 0.28125\n",
            "epoch 4 | 2/28 batches | accuracy 0.484375\n",
            "epoch 4 | 4/28 batches | accuracy 0.390625\n",
            "epoch 4 | 6/28 batches | accuracy 0.578125\n",
            "epoch 4 | 8/28 batches | accuracy 0.546875\n",
            "epoch 4 | 10/28 batches | accuracy 0.46875\n",
            "epoch 4 | 12/28 batches | accuracy 0.375\n",
            "epoch 4 | 14/28 batches | accuracy 0.484375\n",
            "epoch 4 | 16/28 batches | accuracy 0.5\n",
            "epoch 4 | 18/28 batches | accuracy 0.4375\n",
            "epoch 4 | 20/28 batches | accuracy 0.59375\n",
            "epoch 4 | 22/28 batches | accuracy 0.421875\n",
            "epoch 4 | 24/28 batches | accuracy 0.4375\n",
            "epoch 4 | 26/28 batches | accuracy 0.46875\n",
            "------------------------------------------------------------\n",
            "end of epoch 4 | time:0.31113576889038086s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 5 | 0/28 batches | accuracy 0.46875\n",
            "epoch 5 | 2/28 batches | accuracy 0.40625\n",
            "epoch 5 | 4/28 batches | accuracy 0.453125\n",
            "epoch 5 | 6/28 batches | accuracy 0.59375\n",
            "epoch 5 | 8/28 batches | accuracy 0.453125\n",
            "epoch 5 | 10/28 batches | accuracy 0.421875\n",
            "epoch 5 | 12/28 batches | accuracy 0.453125\n",
            "epoch 5 | 14/28 batches | accuracy 0.4375\n",
            "epoch 5 | 16/28 batches | accuracy 0.421875\n",
            "epoch 5 | 18/28 batches | accuracy 0.46875\n",
            "epoch 5 | 20/28 batches | accuracy 0.59375\n",
            "epoch 5 | 22/28 batches | accuracy 0.484375\n",
            "epoch 5 | 24/28 batches | accuracy 0.4375\n",
            "epoch 5 | 26/28 batches | accuracy 0.53125\n",
            "------------------------------------------------------------\n",
            "end of epoch 5 | time:0.2881662845611572s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 6 | 0/28 batches | accuracy 0.3125\n",
            "epoch 6 | 2/28 batches | accuracy 0.484375\n",
            "epoch 6 | 4/28 batches | accuracy 0.3125\n",
            "epoch 6 | 6/28 batches | accuracy 0.546875\n",
            "epoch 6 | 8/28 batches | accuracy 0.5625\n",
            "epoch 6 | 10/28 batches | accuracy 0.578125\n",
            "epoch 6 | 12/28 batches | accuracy 0.546875\n",
            "epoch 6 | 14/28 batches | accuracy 0.390625\n",
            "epoch 6 | 16/28 batches | accuracy 0.5\n",
            "epoch 6 | 18/28 batches | accuracy 0.5\n",
            "epoch 6 | 20/28 batches | accuracy 0.5\n",
            "epoch 6 | 22/28 batches | accuracy 0.46875\n",
            "epoch 6 | 24/28 batches | accuracy 0.4375\n",
            "epoch 6 | 26/28 batches | accuracy 0.359375\n",
            "------------------------------------------------------------\n",
            "end of epoch 6 | time:0.29514169692993164s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 7 | 0/28 batches | accuracy 0.40625\n",
            "epoch 7 | 2/28 batches | accuracy 0.53125\n",
            "epoch 7 | 4/28 batches | accuracy 0.53125\n",
            "epoch 7 | 6/28 batches | accuracy 0.546875\n",
            "epoch 7 | 8/28 batches | accuracy 0.453125\n",
            "epoch 7 | 10/28 batches | accuracy 0.390625\n",
            "epoch 7 | 12/28 batches | accuracy 0.46875\n",
            "epoch 7 | 14/28 batches | accuracy 0.375\n",
            "epoch 7 | 16/28 batches | accuracy 0.46875\n",
            "epoch 7 | 18/28 batches | accuracy 0.5625\n",
            "epoch 7 | 20/28 batches | accuracy 0.34375\n",
            "epoch 7 | 22/28 batches | accuracy 0.390625\n",
            "epoch 7 | 24/28 batches | accuracy 0.46875\n",
            "epoch 7 | 26/28 batches | accuracy 0.578125\n",
            "------------------------------------------------------------\n",
            "end of epoch 7 | time:0.31133508682250977s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 8 | 0/28 batches | accuracy 0.53125\n",
            "epoch 8 | 2/28 batches | accuracy 0.328125\n",
            "epoch 8 | 4/28 batches | accuracy 0.5\n",
            "epoch 8 | 6/28 batches | accuracy 0.4375\n",
            "epoch 8 | 8/28 batches | accuracy 0.46875\n",
            "epoch 8 | 10/28 batches | accuracy 0.4375\n",
            "epoch 8 | 12/28 batches | accuracy 0.5625\n",
            "epoch 8 | 14/28 batches | accuracy 0.34375\n",
            "epoch 8 | 16/28 batches | accuracy 0.40625\n",
            "epoch 8 | 18/28 batches | accuracy 0.53125\n",
            "epoch 8 | 20/28 batches | accuracy 0.484375\n",
            "epoch 8 | 22/28 batches | accuracy 0.515625\n",
            "epoch 8 | 24/28 batches | accuracy 0.4375\n",
            "epoch 8 | 26/28 batches | accuracy 0.546875\n",
            "------------------------------------------------------------\n",
            "end of epoch 8 | time:0.29578447341918945s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 9 | 0/28 batches | accuracy 0.5\n",
            "epoch 9 | 2/28 batches | accuracy 0.53125\n",
            "epoch 9 | 4/28 batches | accuracy 0.484375\n",
            "epoch 9 | 6/28 batches | accuracy 0.578125\n",
            "epoch 9 | 8/28 batches | accuracy 0.515625\n",
            "epoch 9 | 10/28 batches | accuracy 0.390625\n",
            "epoch 9 | 12/28 batches | accuracy 0.4375\n",
            "epoch 9 | 14/28 batches | accuracy 0.421875\n",
            "epoch 9 | 16/28 batches | accuracy 0.390625\n",
            "epoch 9 | 18/28 batches | accuracy 0.46875\n",
            "epoch 9 | 20/28 batches | accuracy 0.390625\n",
            "epoch 9 | 22/28 batches | accuracy 0.484375\n",
            "epoch 9 | 24/28 batches | accuracy 0.59375\n",
            "epoch 9 | 26/28 batches | accuracy 0.40625\n",
            "------------------------------------------------------------\n",
            "end of epoch 9 | time:0.28664326667785645s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 10 | 0/28 batches | accuracy 0.46875\n",
            "epoch 10 | 2/28 batches | accuracy 0.5\n",
            "epoch 10 | 4/28 batches | accuracy 0.421875\n",
            "epoch 10 | 6/28 batches | accuracy 0.390625\n",
            "epoch 10 | 8/28 batches | accuracy 0.40625\n",
            "epoch 10 | 10/28 batches | accuracy 0.484375\n",
            "epoch 10 | 12/28 batches | accuracy 0.515625\n",
            "epoch 10 | 14/28 batches | accuracy 0.5\n",
            "epoch 10 | 16/28 batches | accuracy 0.53125\n",
            "epoch 10 | 18/28 batches | accuracy 0.4375\n",
            "epoch 10 | 20/28 batches | accuracy 0.390625\n",
            "epoch 10 | 22/28 batches | accuracy 0.484375\n",
            "epoch 10 | 24/28 batches | accuracy 0.5\n",
            "epoch 10 | 26/28 batches | accuracy 0.484375\n",
            "------------------------------------------------------------\n",
            "end of epoch 10 | time:0.2986025810241699s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 11 | 0/28 batches | accuracy 0.3125\n",
            "epoch 11 | 2/28 batches | accuracy 0.421875\n",
            "epoch 11 | 4/28 batches | accuracy 0.515625\n",
            "epoch 11 | 6/28 batches | accuracy 0.40625\n",
            "epoch 11 | 8/28 batches | accuracy 0.5\n",
            "epoch 11 | 10/28 batches | accuracy 0.390625\n",
            "epoch 11 | 12/28 batches | accuracy 0.484375\n",
            "epoch 11 | 14/28 batches | accuracy 0.5625\n",
            "epoch 11 | 16/28 batches | accuracy 0.515625\n",
            "epoch 11 | 18/28 batches | accuracy 0.484375\n",
            "epoch 11 | 20/28 batches | accuracy 0.59375\n",
            "epoch 11 | 22/28 batches | accuracy 0.390625\n",
            "epoch 11 | 24/28 batches | accuracy 0.546875\n",
            "epoch 11 | 26/28 batches | accuracy 0.375\n",
            "------------------------------------------------------------\n",
            "end of epoch 11 | time:0.29903292655944824s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 12 | 0/28 batches | accuracy 0.46875\n",
            "epoch 12 | 2/28 batches | accuracy 0.5625\n",
            "epoch 12 | 4/28 batches | accuracy 0.5\n",
            "epoch 12 | 6/28 batches | accuracy 0.453125\n",
            "epoch 12 | 8/28 batches | accuracy 0.453125\n",
            "epoch 12 | 10/28 batches | accuracy 0.640625\n",
            "epoch 12 | 12/28 batches | accuracy 0.390625\n",
            "epoch 12 | 14/28 batches | accuracy 0.546875\n",
            "epoch 12 | 16/28 batches | accuracy 0.5625\n",
            "epoch 12 | 18/28 batches | accuracy 0.40625\n",
            "epoch 12 | 20/28 batches | accuracy 0.40625\n",
            "epoch 12 | 22/28 batches | accuracy 0.375\n",
            "epoch 12 | 24/28 batches | accuracy 0.359375\n",
            "epoch 12 | 26/28 batches | accuracy 0.453125\n",
            "------------------------------------------------------------\n",
            "end of epoch 12 | time:0.3110847473144531s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 13 | 0/28 batches | accuracy 0.4375\n",
            "epoch 13 | 2/28 batches | accuracy 0.578125\n",
            "epoch 13 | 4/28 batches | accuracy 0.4375\n",
            "epoch 13 | 6/28 batches | accuracy 0.46875\n",
            "epoch 13 | 8/28 batches | accuracy 0.40625\n",
            "epoch 13 | 10/28 batches | accuracy 0.34375\n",
            "epoch 13 | 12/28 batches | accuracy 0.46875\n",
            "epoch 13 | 14/28 batches | accuracy 0.53125\n",
            "epoch 13 | 16/28 batches | accuracy 0.34375\n",
            "epoch 13 | 18/28 batches | accuracy 0.5625\n",
            "epoch 13 | 20/28 batches | accuracy 0.515625\n",
            "epoch 13 | 22/28 batches | accuracy 0.484375\n",
            "epoch 13 | 24/28 batches | accuracy 0.453125\n",
            "epoch 13 | 26/28 batches | accuracy 0.546875\n",
            "------------------------------------------------------------\n",
            "end of epoch 13 | time:0.29685521125793457s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 14 | 0/28 batches | accuracy 0.46875\n",
            "epoch 14 | 2/28 batches | accuracy 0.390625\n",
            "epoch 14 | 4/28 batches | accuracy 0.53125\n",
            "epoch 14 | 6/28 batches | accuracy 0.40625\n",
            "epoch 14 | 8/28 batches | accuracy 0.53125\n",
            "epoch 14 | 10/28 batches | accuracy 0.5625\n",
            "epoch 14 | 12/28 batches | accuracy 0.515625\n",
            "epoch 14 | 14/28 batches | accuracy 0.34375\n",
            "epoch 14 | 16/28 batches | accuracy 0.546875\n",
            "epoch 14 | 18/28 batches | accuracy 0.453125\n",
            "epoch 14 | 20/28 batches | accuracy 0.515625\n",
            "epoch 14 | 22/28 batches | accuracy 0.46875\n",
            "epoch 14 | 24/28 batches | accuracy 0.546875\n",
            "epoch 14 | 26/28 batches | accuracy 0.3125\n",
            "------------------------------------------------------------\n",
            "end of epoch 14 | time:0.2978787422180176s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "epoch 15 | 0/28 batches | accuracy 0.4375\n",
            "epoch 15 | 2/28 batches | accuracy 0.421875\n",
            "epoch 15 | 4/28 batches | accuracy 0.421875\n",
            "epoch 15 | 6/28 batches | accuracy 0.484375\n",
            "epoch 15 | 8/28 batches | accuracy 0.53125\n",
            "epoch 15 | 10/28 batches | accuracy 0.46875\n",
            "epoch 15 | 12/28 batches | accuracy 0.515625\n",
            "epoch 15 | 14/28 batches | accuracy 0.5\n",
            "epoch 15 | 16/28 batches | accuracy 0.5625\n",
            "epoch 15 | 18/28 batches | accuracy 0.5\n",
            "epoch 15 | 20/28 batches | accuracy 0.390625\n",
            "epoch 15 | 22/28 batches | accuracy 0.390625\n",
            "epoch 15 | 24/28 batches | accuracy 0.375\n",
            "epoch 15 | 26/28 batches | accuracy 0.53125\n",
            "------------------------------------------------------------\n",
            "end of epoch 15 | time:0.30199384689331055s | valid accuracy 0.4589041095890411\n",
            "------------------------------------------------------------\n",
            "Elapsed time 4.497542381286621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vIytcEn4YZB"
      },
      "source": [
        "## 3 Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRjEGyEn4ZB2",
        "outputId": "e6ae2f01-4da1-42c3-abdb-66ba4d87e039"
      },
      "source": [
        "print('Check results of the test dataset')\n",
        "pre_acc_test = eval_model(pre_news_model,pre_test_dataloader, criterion)\n",
        "print(f'Test accuracy {pre_acc_test}')"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check results of the test dataset\n",
            "Test accuracy 0.4589041095890411\n"
          ]
        }
      ]
    }
  ]
}